{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Testing and CI\n",
    "\n",
    "\n",
    "The notebook contains problems about code testing and continuous integration with Travis CI.\n",
    "\n",
    "* * *\n",
    "\n",
    "Original by E Tollerud 2017 for LSSTC DSFP Session3 and AstroHackWeek, modified by B Sipocz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Set up py.test in you repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we'll aim to get the [py.test](https://docs.pytest.org/en/latest/) testing framework up and running in the code repository you set up in the last set of problems.  We can then use it to collect and run tests of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a: Ensure py.test is installed\n",
    "\n",
    "Of course ``py.test`` must actually be installed before you can use it. The commands below should work for the Anaconda Python Distribution, but if you have some other Python installation you'll want to install `pytest` (and its coverage plugin) as directed in the install instructions for ``py.test``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: / \n",
      "WARNING: The remote server could not find the noarch directory for the\n",
      "requested channel with url: http://ssb.stsci.edu/astrocondo\n",
      "\n",
      "It is possible you have given conda an invalid channel. Please double-check\n",
      "your conda configuration using `conda config --show`.\n",
      "\n",
      "If the requested url is in fact a valid conda channel, please request that the\n",
      "channel administrator create `noarch/repodata.json` and associated\n",
      "`noarch/repodata.json.bz2` files, even if `noarch/repodata.json` is empty.\n",
      "$ mkdir noarch\n",
      "$ echo '{}' > noarch/repodata.json\n",
      "$ bzip2 -k noarch/repodata.json\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.4.10\n",
      "  latest version: 4.5.11\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install pytest pytest-cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b: Ensure your repo has code suitable for unit tests\n",
    "\n",
    "Depending on what your code actually does, you might need to modify it to actually perform something testable.  For example, if all it does is print something, you might find it difficult to write an effective unit test.  Try adding a function that actually performs some operation and returns something different depending on various inputs.  That tends to be the easiest function to unit-test: one with a clear \"right\" answer in certain situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also be sure you have `cd`ed to the *root* of the repo for `pytest` to operate correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions/Session7/Day1\n",
      "/Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions/Session7/Day1\r\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions/Session7/Day1/\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c: Add a test file with a test function\n",
    "\n",
    "The test must be part of the package and follow the convention that the file and the function begin with ``test`` to get picked up by the test collection machinery.  Inside the test function, you'll need some code that fails if the test condition fails.  The easiest way to do this is with an ``assert`` statement, which raises an error if its first argument is False.\n",
    "\n",
    "*Hint: remember that to be a valid python package, a directory must have an ``__init__.py``*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: test_code: No such file or directory\n",
      "/Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions/Session7/Day1/test_code/tests/test_code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%touch` not found.\n"
     ]
    }
   ],
   "source": [
    "!rm -r test_code\n",
    "%mkdir test_code\n",
    "%cd test_code\n",
    "%touch __init__.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing testcode.py\n"
     ]
    }
   ],
   "source": [
    "%%file testcode.py\n",
    "\n",
    "import math\n",
    "\n",
    "def even(x):\n",
    "    c = math.ceil(x)\n",
    "    f = math.floor(x)\n",
    "    if c%2 == 0:\n",
    "        r = c\n",
    "    else:\n",
    "        r = f\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: tests: File exists\n",
      "testcode.py \u001b[34mtests\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%mkdir tests\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-75-8b0f9dcc098e>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-75-8b0f9dcc098e>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    %%file test_testcode.py\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%cd tests\n",
    "\n",
    "%%file test_testcode.py\n",
    "\n",
    "def test_even(x):\n",
    "    s = even(x)\n",
    "    assert s%2 == 0\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions\n",
      "LICENSE.txt \u001b[34mSession1\u001b[m\u001b[m    \u001b[34mSession3\u001b[m\u001b[m    \u001b[34mSession5\u001b[m\u001b[m    \u001b[34mSession7\u001b[m\u001b[m\r\n",
      "README.md   \u001b[34mSession2\u001b[m\u001b[m    \u001b[34mSession4\u001b[m\u001b[m    \u001b[34mSession6\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions/Session7/Day1\n"
     ]
    }
   ],
   "source": [
    "%cd Session7/Day1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BriefIntroToMachineLearning.ipynb \u001b[34mimages\u001b[m\u001b[m\r\n",
      "Code repositories.ipynb           \u001b[34mlsst_foo\u001b[m\u001b[m\r\n",
      "Code testing and CI.ipynb         \u001b[34mnew_dir\u001b[m\u001b[m\r\n",
      "MachLearnAstroData.ipynb          sdss_training_set.h5\r\n",
      "README.md                         \u001b[34mtest_code\u001b[m\u001b[m\r\n",
      "Untitled.ipynb                    \u001b[34mtest_repo\u001b[m\u001b[m\r\n",
      "__init__.py                       testcode.py\r\n",
      "blind_test_set.h5                 \u001b[34mtests\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d: Run the test directly\n",
    "\n",
    "While this is not how you'd ordinarily run the tests, it's instructive to first try to execute the test *directly*, without using any fancy test framework.  If your test function just runs, all is good.  If you get an exception, the test failed (which in this case might be *good*).\n",
    "\n",
    "*Hint: you may need to use `reload` or just re-start your notebook kernel to get the cell below to recognize the changes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'test_code.tests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f8a9e4d21b7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtest_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest_testcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_testcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_even\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'test_code.tests'"
     ]
    }
   ],
   "source": [
    "from test_code.tests import test_testcode\n",
    "test_testcode.test_even(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e: Run the tests with ``py.test``\n",
    "\n",
    "Once you have an example test, you can try invoking ``py.test``, which is how you should run the tests in the future.  This should yield a report that shows a dot for each test. If all you see are dots, the tests ran sucessfully.  But if there's a failure, you'll see the error, and the traceback showing where the error happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.6.4, pytest-3.3.2, py-1.5.2, pluggy-0.6.0\n",
      "rootdir: /Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions/Session7/Day1, inifile:\n",
      "plugins: cov-2.6.0\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_code/tests/test_testcode.py E\u001b[36m                                       [100%]\u001b[0m\n",
      "\n",
      "==================================== ERRORS ====================================\n",
      "_________________________ ERROR at setup of test_even __________________________\n",
      "file /Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions/Session7/Day1/test_code/tests/test_testcode.py, line 3\n",
      "  def test_even(x):\n",
      "\u001b[31mE       fixture 'x' not found\u001b[0m\n",
      "\u001b[31m>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, doctest_namespace, monkeypatch, no_cover, pytestconfig, record_xml_property, recwarn, tmpdir, tmpdir_factory\u001b[0m\n",
      "\u001b[31m>       use 'pytest --fixtures [testpath]' for help on them.\u001b[0m\n",
      "\n",
      "/Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions/Session7/Day1/test_code/tests/test_testcode.py:3\n",
      "\u001b[31m\u001b[1m=========================== 1 error in 0.05 seconds ============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1f: Make the test fail (or succeed...)\n",
    "\n",
    "If your test failed when you ran it, you should now try to fix the test (or the code...) to make it work.  Try running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Modify your test to fail if it succeeded before, or vice versa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.6.4, pytest-3.3.2, py-1.5.2, pluggy-0.6.0\r\n",
      "rootdir: /Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions/Session7/Day1, inifile:\r\n",
      "plugins: cov-2.6.0\r\n",
      "\u001b[1m\r",
      "collecting 0 items                                                             \u001b[0m\u001b[1m\r",
      "collecting 1 item                                                              \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "test_code/tests/test_testcode.py E\u001b[36m                                       [100%]\u001b[0m\r\n",
      "\r\n",
      "==================================== ERRORS ====================================\r\n",
      "_________________________ ERROR at setup of test_even __________________________\r\n",
      "file /Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions/Session7/Day1/test_code/tests/test_testcode.py, line 3\r\n",
      "  def test_even(x):\r\n",
      "\u001b[31mE       fixture 'x' not found\u001b[0m\r\n",
      "\u001b[31m>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, doctest_namespace, monkeypatch, no_cover, pytestconfig, record_xml_property, recwarn, tmpdir, tmpdir_factory\u001b[0m\r\n",
      "\u001b[31m>       use 'pytest --fixtures [testpath]' for help on them.\u001b[0m\r\n",
      "\r\n",
      "/Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions/Session7/Day1/test_code/tests/test_testcode.py:3\r\n",
      "\u001b[31m\u001b[1m=========================== 1 error in 0.02 seconds ============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!py.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1g: Check coverage\n",
    "\n",
    "The coverage plugin we installed will let you check which lines of your code are actually run by the testing suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: yourproject: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!py.test --cov=<yourproject> tests/ #complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should yield a report, which you can use to decide if you need to add more tests to acheive complete coverage.  Check out the command line arguments to see if you can get a more detailed line-by-line report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Implement some unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sub-problems below each contain different unit testing complications.  Place the code from the snippets in your repository (either using an editor or the ``%%file`` trick), and write tests to ensure the correctness of the functions.  Try to achieve 100% coverage for all of them (especially to catch some hidden bugs!).\n",
    "\n",
    "Also, note that some of these examples are not really practical - that is, you wouldn't want to do this in *real* code because there's better ways to do it.  But because of that, they are good examples of where something can go subtly wrong... and therefore where you want to make tests!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a\n",
    "\n",
    "When you have a function with a default, it's wise to test both the with-default call (``function_b()``), and when you give a value (``function_b(1.2)``)\n",
    "\n",
    "*Hint: Beware of numbers that come close to 0... write your tests to accomodate floating-point errors!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file <yourproject>/<filename>.py #complete, or just use your editor\n",
    "\n",
    "# `math` here is for *scalar* math... normally you'd use numpy but this makes it a bit simpler to debug\n",
    "\n",
    "import math \n",
    "\n",
    "inf = float('inf')  # this is a quick-and-easy way to get the \"infinity\" value \n",
    "\n",
    "def function_a(angle=180):\n",
    "    anglerad = math.radians(angle)\n",
    "    return math.sin(anglerad/2)/math.sin(anglerad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b\n",
    "\n",
    "This test has an intentional bug... but depending how you right the test you *might* not catch it...  Use unit tests to find it! (and then fix it...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file <yourproject>/<filename>.py #complete, or just use your editor\n",
    "\n",
    "def function_b(value):\n",
    "    if value < 0:\n",
    "        return value - 1\n",
    "    else:\n",
    "        value2 = subfunction_b(value + 1)\n",
    "        return value + value2\n",
    "    \n",
    "def subfunction_b(inp):\n",
    "    vals_to_accum = []\n",
    "    for i in range(10):\n",
    "        vals_to_accum.append(inp ** (i/10))\n",
    "    if vals_to_accum[-1] > 2:\n",
    "        vals.append(100)\n",
    "    # really you would use numpy to do this kind of number-crunching... but we're doing this for the sake of example right now\n",
    "    return sum(vals_to_accum) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c\n",
    "\n",
    "There are (at least) *two* significant bugs in this code (one fairly apparent, one much more subtle).  Try to catch them both, and write a regression test that covers those cases once you've found them.\n",
    "\n",
    "One note about this function: in real code you're probably better off just using [the Angle object from `astropy.coordinates`](http://docs.astropy.org/en/stable/coordinates/angles.html).  But this example demonstrates one of the reasons *why* that was created, as it's very easy to write a buggy version of this code.\n",
    "\n",
    "*Hint: you might find it useful to use `astropy.coordinates.Angle` to create test cases...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file <yourproject>/<filename>.py #complete, or just use your editor\n",
    "\n",
    "import math\n",
    "\n",
    "# know that to not have to worry about this, you should just use `astropy.coordinates`.\n",
    "def angle_to_sexigesimal(angle_in_degrees, decimals=3):\n",
    "    \"\"\"\n",
    "    Convert the given angle to a sexigesimal string of hours of RA.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    angle_in_degrees : float\n",
    "        A scalar angle, expressed in degrees\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    hms_str : str\n",
    "        The sexigesimal string giving the hours, minutes, and seconds of RA for the given `angle_in_degrees`\n",
    "        \n",
    "    \"\"\"\n",
    "    if math.floor(decimals) != decimals:\n",
    "        raise ValueError('decimals should be an integer!')\n",
    "    \n",
    "    hours_num = angle_in_degrees*24/180\n",
    "    hours = math.floor(hours_num)\n",
    "    \n",
    "    min_num = (hours_num - hours)*60\n",
    "    minutes = math.floor(min_num)\n",
    "    \n",
    "    seconds = (min_num - minutes)*60\n",
    "\n",
    "    format_string = '{}:{}:{:.' + str(decimals) + 'f}'\n",
    "    return format_string.format(hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d\n",
    "\n",
    "*Hint: numpy has some useful functions in [numpy.testing](https://docs.scipy.org/doc/numpy/reference/routines.testing.html) for comparing arrays.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file <yourproject>/<filename>.py #complete, or just use your editor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def function_d(array1=np.arange(10)*2, array2=np.arange(10), operation='-'):\n",
    "    \"\"\"\n",
    "    Makes a matrix where the [i,j]th element is array1[i] <operation> array2[j]\n",
    "    \"\"\"\n",
    "    if operation == '+':\n",
    "        return array1[:, np.newaxis] + array2\n",
    "    elif operation == '-':\n",
    "        return array1[:, np.newaxis] - array2\n",
    "    elif operation == '*':\n",
    "        return array1[:, np.newaxis] * array2\n",
    "    elif operation == '/':\n",
    "        return array1[:, np.newaxis] / array2\n",
    "    else:\n",
    "        raise ValueError('Unrecognized operation \"{}\"'.format(operation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Set up travis to run your tests whenever a change is made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a testing suite set up, you can try to turn on a continuous integration service to constantly check that any update you might send doesn't create a bug.  We will the [Travis-CI](https://travis-ci.org/) service for this purpose, as it has one of the lowest barriers to entry from Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a: Ensure the test suite is passing locally\n",
    "\n",
    "Seems obvious, but it's easy to forget to check this and only later realize that all the trouble you thought you had setting up the CI service was because the tests were actually broken..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.6.4, pytest-3.3.2, py-1.5.2, pluggy-0.6.0\n",
      "rootdir: /Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions/Session7/Day1, inifile:\n",
      "plugins: cov-2.6.0\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_code/tests/test_testcode.py E\u001b[36m                                       [100%]\u001b[0m\n",
      "\n",
      "==================================== ERRORS ====================================\n",
      "_________________________ ERROR at setup of test_even __________________________\n",
      "file /Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions/Session7/Day1/test_code/tests/test_testcode.py, line 3\n",
      "  def test_even(x):\n",
      "\u001b[31mE       fixture 'x' not found\u001b[0m\n",
      "\u001b[31m>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, doctest_namespace, monkeypatch, no_cover, pytestconfig, record_xml_property, recwarn, tmpdir, tmpdir_factory\u001b[0m\n",
      "\u001b[31m>       use 'pytest --fixtures [testpath]' for help on them.\u001b[0m\n",
      "\n",
      "/Users/prabhani/DSFP/GIT/LSSTC-DSFP-Sessions/Session7/Day1/test_code/tests/test_testcode.py:3\n",
      "\u001b[31m\u001b[1m=========================== 1 error in 0.05 seconds ============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b: Set up an account on travis\n",
    "\n",
    "This turns out to be quite convenient.  If you go to the [Travis web site](https://travis-ci.org/), you'll see a \"Sign in with GitHub\" button.  You'll need to authorize Travis, but once you've done so it will automatically log you in and know which repositories are yours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c: Create a minimal ``.travis.yml`` file.\n",
    "\n",
    "Before we can activate travis on our repo, we need to tell travis a variety of metadata about what's in the repository and how to run it.  The template below should be sufficient for the simplest needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%file .travis.yml\n",
    "\n",
    "language: python\n",
    "python:\n",
    "  - \"3.6\"\n",
    "# command to install dependencies\n",
    "#install: \"pip install numpy\"  #uncomment this if your code depends on numpy or similar\n",
    "# command to run tests\n",
    "script: pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to commit and push this to github before proceeding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!git #complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d: activate Travis\n",
    "\n",
    "You can now click on your profile picture in the upper-right and choose \"accounts\".  You should see your repo listed there, presumably with a grey X next to it.  Click on the X, which should slide the button over and therefore activate travis on that repository. Once you've done this, you should be able to click on the name of the reposository in the travis accounts dashboard, popping up a window showing the build already in progress (if not, just be a bit patient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Wait for the tests to complete.  If all is good you should see a green check next to the repository name.  Otherwise you'll need to go in and fix it and the tests will automatically trigger when you send a new update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e: Break the build\n",
    "\n",
    "Make a small change to the repository to break a test.  If all else fails simply add the following test:\n",
    "\n",
    "```\n",
    "def test_fail():\n",
    "    assert False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Push that change up and go look at travis.  It should automatically run the tests and result in them failing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3f: Have your neighbor fix your repo\n",
    "\n",
    "Challenge your nieghbor to find the bug and fix it.  Have them follow the Pull Request workflow, but do *not* merge the PR until Travis' tests have finished (they *should* run automatically, and leave note in the github PR page to that effect).  Once the tests have finished, they will tell you if the fix really does cure the bug.  If it does, merge it and say thank you.  If it doesn't, ask your neighbor to try updating their fix with the feedback from Travis...\n",
    "\n",
    "*Hint: it may be late in the day, but keep being nice!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem 1: Use py.test \"parametrization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``py.test`` has a feature called test parametrization that can be extremely useful for writing easier-to-understand tests. The key idea is that you can use one simple test *function*, but with multiple inputs, and break that out into separate tests.  At first glance this might appear similar to just one test where you interate over lots of inputs, but it's actually much more useful because it doesn't stop at the *first* failure.  Rather it will run all the inputs ever time, helpinf you debug subtle problems where only certain inputs fail.\n",
    "\n",
    "For more info and how to actually *use* the feature, see [the py.test docs on the subject](https://docs.pytest.org/en/latest/parametrize.html).  In this challenge problem, try adapting the Problem 2 cases to use this feature. 2c and 2d are particularly amenable to this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem 2: Test-driven development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-driven development is a radically different approach to designing code from what we're generally used to.  In test-driven design, you write the tests *first*.  That is, you write how you expect your code to behave before writing the code.\n",
    "\n",
    "For this problem, try experimenting with test-driven desgin.  Choose a problem (ideally from your science interests) where you know some clear cases that you could write tests for.  Write the full testing suite (using the techniques you developed above).  Then run the tests to ensure all the new ones are  failing due to lack of implementation, and then write the new code.  A few ideas are given below, but, again, for a real challenge try to come up with your own problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the location of Lagrange points for two arbitrary mass bodies.  (Good test cases are the Earth-Moon or Earth-Sun system, which you can probably find on wikipedia.)  Consider solving the problem numerically instead of with formulae you can look up, but use the formulae to concoct the test cases.\n",
    "* Write a function that uses one of the a clustering algorithm in [scikit-learn](http://scikit-learn.org/stable/modules/clustering.html) to identify the centers of two 2D gaussian point-clouds.  The tests are particularly easy to formulate before-hand because you know the right answer at the outset if you generate the point-clouds yourself."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
